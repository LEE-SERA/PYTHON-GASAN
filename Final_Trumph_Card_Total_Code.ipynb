{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 트럼프 트위터 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GetOldTweets3 as got\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "\n",
    "days_range = [] # 날짜 범위를 담을 list 생성\n",
    "\n",
    "start = datetime.datetime.strptime(\"2017-01-01\", \"%Y-%m-%d\") # datetime 라이브러리 크롤링 시작 날짜 설정\n",
    "end = datetime.datetime.strptime(\"2017-01-31\", \"%Y-%m-%d\") # datetime 라이브러리 크롤링 끝나는 날짜 설정\n",
    "date_generated = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days)]\n",
    "# 시작 날짜와 0부터 끝나는 날짜와의 차이를 추가함\n",
    "# timedelta 함수는 추가함\n",
    "\n",
    "for date in date_generated: # 시작 날짜부터 끝나는 날짜까지 for문 돌린다\n",
    "    days_range.append(date.strftime(\"%Y-%m-%d\")) # days_range 리스트에 시작날짜부터 끝나는 날짜까지 추가\n",
    "\n",
    "print(\"=== 설정된 트윗 수집 기간은 {} 에서 {} 까지 입니다 ===\".format(days_range[0], days_range[-1])) # 시작날짜~끝나는 날짜 출력\n",
    "print(\"=== 총 {}일 간의 데이터 수집 ===\".format(len(days_range))) # 데이터 수집 기간 출력\n",
    "\n",
    "# 특정 검색어가 포함된 트윗 검색하기 (quary search)\n",
    "# 검색어 : 어벤져스, 스포\n",
    "\n",
    "import time\n",
    "\n",
    "# 수집 기간 맞추기\n",
    "start_date = days_range[0] # days_range의 0번 째 인덱스는 시작 날짜\n",
    "end_date = (datetime.datetime.strptime(days_range[-1], \"%Y-%m-%d\")  # day_range의 마지막 인덱스에\n",
    "            + datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")  # 1일을 더해줌\n",
    "# setUntil이 끝을 포함하지 않으므로, day + 1\n",
    "\n",
    "# 트윗 수집 기준 정의\n",
    "tweetCriteria = got.manager.TweetCriteria().setUsername('@realDonaldTrump')\n",
    "# GetOldTweet3 클래스 의 객체 got의 setUsername 함수 사용해서 계정 명 \n",
    "                                           .setSince(start_date)\n",
    "# GetOldTweet3 클래스 의 객체 got의 setUsername 함수 사용해서 시작날짜\n",
    "                                           .setUntil(end_date)\n",
    "# GetOldTweet3 클래스 의 객체 got의 setUsername 함수 사용해서 끝나는 날짜\n",
    "                                           \n",
    "\n",
    "# 수집 with GetOldTweet3\n",
    "print(\"Collecting data start.. from {} to {}\".format(days_range[0], days_range[-1])) # 수집 시작하면 시작일부터 종료일까지 출력\n",
    "start_time = time.time() # 시작 시간을 컴퓨터의 현재 시각으로 설정\n",
    "\n",
    "tweet = got.manager.TweetManager.getTweets(tweetCriteria) # getoldtweet3 클래스의 getTweets 함수로 수집 시작\n",
    "\n",
    "print(\"Collecting data end.. {0:0.2f} Minutes\".format((time.time() - start_time)/60)) # 시작 시간과 현재 시간의 차이를 수행 시간으로 출력\n",
    "print(\"=== Total num of tweets is {} ===\".format(len(tweet))) # 수집한 tweet의 갯수 출력\n",
    "\n",
    "\n",
    "# 원하는 변수 골라서 저장하기\n",
    "\n",
    "from random import uniform\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# initialize\n",
    "tweet_list = [] # tweet_list 생성\n",
    "\n",
    "for index in tqdm_notebook(tweet): # tweet의 진행 상황 표시해주는 tqdm_notebook \n",
    "    \n",
    "    # 메타데이터 목록 \n",
    "    username = index.username # tweet의 username 저장\n",
    "    content = index.text # tweet의 text 저장\n",
    "    tweet_date = index.date.strftime(\"%Y-%m-%d\") # tweet의 날짜를 문자열로 저장\n",
    "    tweet_time = index.date.strftime(\"%H:%M:%S\") # tweet의 시간을 문자열로 저장\n",
    "    \n",
    "    \n",
    "    # 결과 합치기\n",
    "    info_list = [tweet_date, tweet_time, username, content] # info_list라는 변수에 날짜,시간,유저이름,트윗 내용을 리스트화하여 저장\n",
    "    tweet_list.append(info_list) # tweet_list 리스트에 info_list 추가함\n",
    "    \n",
    "    # 휴식 \n",
    "  #  time.sleep(uniform(1,2))\n",
    "\n",
    "# 파일 저장하기\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "twitter_df = pd.DataFrame(tweet_list,  # tweet_list 로 twitter_df 라는 dateframe 생성\n",
    "                          columns = [\"date\", \"time\", \"user_name\", \"text\"])\n",
    "\n",
    "# csv 파일 만들기\n",
    "twitter_df.to_csv(\"sample_twitter_data_{}_to_{}.csv\".format(days_range[0], days_range[-1]), index=False)\n",
    "# twitter_df를 sample_twitter_data_시작일_to_종료일.csv로 저장\n",
    "print(\"=== {} tweets are successfully saved ===\".format(len(tweet_list)))\n",
    "# tweet 갯수 tweets are succesfully saved 출력\n",
    "\n",
    "\n",
    "# 파일 확인하기\n",
    "\n",
    "df_tweet = pd.read_csv('sample_twitter_data_{}_to_{}.csv'.format(days_range[0], days_range[-1]))\n",
    "# pandas의 read_csv 함수로 twitter_df 파일 읽어옴\n",
    "df_tweet.head(10) # 위에서 10개만 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 트럼프 크롤링 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 데이터 불러오기\n",
    "\n",
    "df = pd.read_csv('./Trumph_17_19.csv')\n",
    "\n",
    "# 언어 처리\n",
    "\n",
    "text_new = []\n",
    "for i in range(len(df)):\n",
    "    df1 = df.iloc[i:i+1,]\n",
    "    text_list = df1['text'].tolist()\n",
    "\n",
    "    content_text=''\n",
    "    for each_line in text_list:\n",
    "        content_text = content_text + str(each_line) + '\\n'\n",
    "    content_text = content_text.replace('\\n','')\n",
    "    content_text = content_text.replace(\"\\\\\",'') \n",
    "    content_text = content_text.replace(\"\\'\",'\"')   \n",
    "    \n",
    "    remove_list=[\"haven't\", 'haven\"t', \"hadn't\", 'hadn\"t', \"hasn't\", 'hasn\"t',\n",
    "                 \"didn't\", 'didn\"t', \"doesn't\", 'doesn\"t', \"don't\", 'don\"t',\n",
    "                 \"wasn't\", 'wasn\"t', \"weren't\", 'weren\"t', \"isn't\", 'isn\"t',\n",
    "                 \"aren't\", 'aren\"t', \"wouldn't\", 'wouldn\"t', \"won't\", 'won\"t',\n",
    "                 \"couldn't\", 'couldn\"t', \"shouldn't\", 'shouldn\"t', \"needn't\",\n",
    "                 'needn\"t', \"mightn't\", 'mightn\"t', \"mustn't\", 'mustn\"t', \"ain't\",\n",
    "                 'ain\"t', \"shan't\", 'shan\"t', \"can't\", 'can\"t']\n",
    "    \n",
    "    for i in remove_list:\n",
    "        if i in content_text:\n",
    "            content_text = content_text.replace(i, 'not')\n",
    "    \n",
    "    only_english = re.sub('[^a-zA-Z]', ' ', content_text)\n",
    "    \n",
    "    while '  ' in only_english:\n",
    "        only_english = only_english.replace('  ', ' ')\n",
    "        \n",
    "    no_capitals = only_english.lower().split()\n",
    "    \n",
    "    stops = set(stopwords.words('english'))\n",
    "    stops.add('www')\n",
    "    stops.add('https')\n",
    "    stops.remove('not')\n",
    "    \n",
    "    no_stops = [word for word in no_capitals if not word in stops]\n",
    "    \n",
    "    stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    stemmer_words = [stemmer.stem(word) for word in no_stops]\n",
    "    \n",
    "    text_join = ' '.join(stemmer_words)\n",
    "    text_new.append(text_join)\n",
    "\n",
    "df['text_new'] = text_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIMES 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Times 데이터 수집\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "URL='https://www.nytimes.com/search?dropmab=true&endDate=20200102&query=trumph%20&sections=World%7Cnyt%3A%2F%2Fsection%2F70e865b6-cc70-5181-84c9-8368b3a5c34b&sort=oldest&startDate=20170609&types=article'\n",
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "driver.get(URL)\n",
    "\n",
    "title_list=[] ; date_list =[]; subtitle_list=[]\n",
    "\n",
    "\n",
    "while ('Dec 31, 2017' not in date_list):\n",
    "    \n",
    "    for i in range(4):\n",
    "        #기사 제목 수집\n",
    "        Title = driver.find_elements_by_class_name('css-2fgx4k')\n",
    "    \n",
    "        time.sleep(0.05)\n",
    "        #기사 날짜 수집    \n",
    "        date = driver.find_elements_by_class_name('css-17ubb9w')\n",
    "    \n",
    "        time.sleep(0.05)\n",
    "        #기사 머릿말 수집\n",
    "        subtitle = driver.find_elements_by_class_name('css-16nhkrn')\n",
    "    \n",
    "        time.sleep(0.05)\n",
    "    \n",
    "        for i in range(len(Title)):\n",
    "            if(Title[i].text not in title_list):\n",
    "                    \n",
    "                title_list.append(Title[i].text)\n",
    "                date_list.append(date[i].text)\n",
    "                subtitle_list.append(subtitle[i].text)\n",
    "    \n",
    "        Title.clear()\n",
    "        date.clear()\n",
    "        subtitle.clear()\n",
    "    \n",
    "        #스크롤 내리기\n",
    "        elem= driver.find_element_by_tag_name('body')\n",
    "        elem.send_keys(Keys.PAGE_DOWN)\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "    #SHOW MORE 클릭\n",
    "    driver.find_element_by_xpath('//*[@id=\"site-content\"]/div/div[2]/div[2]/div/button').click()\n",
    "    \n",
    "d=date_list.copy()\n",
    "t=title_list.copy()\n",
    "st=subtitle_list.copy()\n",
    "\n",
    "#기사 제목, 머릿말 결합\n",
    "for k in range(len(t)):\n",
    "    t[k]=t[k]+' '+st[k]\n",
    "    \n",
    "dc={'date':d, 'text':t}\n",
    "\n",
    "data = pd.DataFrame(dc)\n",
    "data.to_csv('20180101_20180630.csv', mode='w')\n",
    "\n",
    "#Times 데이터 불러오기\n",
    "one = pd.read_csv('20170101_20170630.csv')\n",
    "two = pd.read_csv('20170608_20171231.csv')\n",
    "three = pd.read_csv('20180101_20180701.csv')\n",
    "four = pd.read_csv('20180701_20181231.csv')\n",
    "five = pd.read_csv('20190101_20190630.csv')\n",
    "six = pd.read_csv('20190605_20191231.csv', header=None)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "#기사 날짜 정규 날짜 표현으로 바꿈\n",
    "def change_date_form(date):\n",
    "    if(date[:3]=='Jan'):\n",
    "        return (date[-4:]+'-'+'01'+'-'+date[5:7]).replace(',','')\n",
    "    if(date[:3]=='Feb'):\n",
    "        return (date[-4:]+'-'+'02'+'-'+date[5:7]).replace(',','')\n",
    "    if(date[:3]=='Mar'):\n",
    "        return (date[-4:]+'-'+'03'+'-'+date[6:8]).replace(',','')\n",
    "    if(date[:3]=='Apr'):\n",
    "        return (date[-4:]+'-'+'04'+'-'+date[6:8]).replace(',','')\n",
    "    if(date[:3]=='May'):\n",
    "        return (date[-4:]+'-'+'05'+'-'+date[4:6]).replace(',','')\n",
    "    if(date[:3]=='Jun'):\n",
    "        return (date[-4:]+'-'+'06'+'-'+date[5:7]).replace(',','')\n",
    "    if(date[:3]=='Jul'):\n",
    "        return (date[-4:]+'-'+'07'+'-'+date[5:7]).replace(',','')\n",
    "    if(date[:3]=='Aug'):\n",
    "        return (date[-4:]+'-'+'08'+'-'+date[5:7]).replace(',','')\n",
    "    if(date[:3]=='Sep'):\n",
    "        return (date[-4:]+'-'+'09'+'-'+date[6:8]).replace(',','')\n",
    "    if(date[:3]=='Oct'):\n",
    "        return (date[-4:]+'-'+'10'+'-'+date[5:7]).replace(',','')\n",
    "    if(date[:3]=='Nov'):\n",
    "        return (date[-4:]+'-'+'11'+'-'+date[5:7]).replace(',','')\n",
    "    if(date[:3]=='Dec'):\n",
    "        return (date[-4:]+'-'+'12'+'-'+date[5:7]).replace(',','')  \n",
    "\n",
    "#Times 데이터 하나로 결합\n",
    "data=pd.concat([one,two,three,four,five,six])\n",
    "\n",
    "data['date'].apply(change_date_form)\n",
    "for i in range(len(uu)):\n",
    "    data['date'][i]=datetime.datetime.strptime(data['date'][i],'%Y-%m-%d').date().strftime('%Y-%m-%d')\n",
    "\n",
    "# 저장\n",
    "data.to_csv('Times_data.csv',mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIMES 전처리\n",
    "- TFIDF 모델 전처리 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF 모델 전처리 코드\n",
    "import pandas as pd\n",
    "times = pd.read_csv('Times_data_수정.csv')\n",
    "trump = pd.read_csv('Trumph_Kospi_token.csv')\n",
    "\n",
    "times=times[['date','text_new']]\n",
    "times.columns=['date','text']\n",
    "\n",
    "#형태소 분석 라이브러리\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer ; tknzr = TweetTokenizer()\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def lower_change(text):\n",
    "    return text.lower()\n",
    "times['text']=times['text'].apply(lower_change)\n",
    "\n",
    "Tdate=set(times['date'])\n",
    "Tdate=sorted(list(Tdate))\n",
    "text=[]\n",
    "for j in range(len(Tdate)):\n",
    "    text.append(str(times[times['date']==Tdate[j]]['text'].values))\n",
    "for k in range(len(text)):\n",
    "    text[k] = text[k].replace('[','')\n",
    "    text[k] = text[k].replace(']','')\n",
    "\n",
    "dc={'date':Tdate, 'text':text}\n",
    "Times = pd.DataFrame(dc)\n",
    "\n",
    "#각 트위터 날짜 기준으로 3일 전, 3일 후의 Times 데이터를 합쳐줌\n",
    "times_text=[]\n",
    "timesdate=[s for s in Times['date']]\n",
    "for x in range(len(trump['date'])):\n",
    "    \n",
    "    before = (parse(trump['date'][x])-datetime.timedelta(days=2)).strftime('%Y-%m-%d')\n",
    "    after = (parse(trump['date'][x])+datetime.timedelta(days=2)).strftime('%Y-%m-%d')\n",
    "    box_text=[]\n",
    "    Ttime=trump['date'][x]\n",
    "    \n",
    "    #3일 전 데이터가 있으면 3일 전 부터 해당 트위터 날짜까지 Times데이터를 가져옴\n",
    "    if( before in timesdate or (parse(before)-datetime.timedelta(days=1)).strftime('%Y-%m-%d') in timesdate):\n",
    "        while(Ttime not in timesdate):\n",
    "            Ttime=(parse(Ttime)-datetime.timedelta(days=1)).strftime('%Y-%m-%d') \n",
    "        \n",
    "        while(before not in timesdate):\n",
    "            before=(parse(before)-datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        for y in range(timesdate.index(before), timesdate.index(Ttime)+1):\n",
    "            box_text.append(Times['text'][y])\n",
    "           \n",
    "    #3일 전 데이터가 없으면 해당 트위터 날짜 전까지의 Times데이터를 가져옴\n",
    "    else:\n",
    "        while(Ttime not in timesdate):\n",
    "            Ttime=(parse(Ttime)-datetime.timedelta(days=1)).strftime('%Y-%m-%d') \n",
    "            \n",
    "        for z in range(timesdate.index(Ttime)+1):\n",
    "            box_text.append(Times['text'][z])\n",
    "            \n",
    "    #3일 후 데이터가 있으면 해당 트위터 날짜부터 3일 후의 Times데이터를 가져옴\n",
    "    if( after in timesdate or (parse(after)+datetime.timedelta(days=1)).strftime('%Y-%m-%d') in timesdate):\n",
    "        while(Ttime not in timesdate):\n",
    "            Ttime=(parse(Ttime)+datetime.timedelta(days=1)).strftime('%Y-%m-%d') \n",
    "            \n",
    "        while(after not in timesdate):\n",
    "            after=(parse(after)-datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        for yy in range(timesdate.index(after), timesdate.index(Ttime)+1):\n",
    "            box_text.append(Times['text'][yy])\n",
    "            \n",
    "    #2일 후 데이터가 없으면 해당 트위터 날짜부터 끝까지 Times데이터를 가져옴\n",
    "    else:\n",
    "        while(Ttime not in timesdate):\n",
    "            Ttime=(parse(Ttime)+datetime.timedelta(days=1)).strftime('%Y-%m-%d') \n",
    "            \n",
    "        for zz in range(timesdate.index(Ttime),len(timesdate)):\n",
    "            box_text.append(Times['text'][zz])\n",
    "    \n",
    "    times_text.append(box_text)\n",
    "\n",
    "trump_date=[v for v in trump['date']]\n",
    "Times_2days={'date':trump_date, 'text':times_text}\n",
    "times_2days=pd.DataFrame(Times_2days)\n",
    "\n",
    "#불용어 처리\n",
    "stop_words=['[',']','\\'','\\\"','\"\"','\\' ',',','.','\\'''','\\\\',':','-','\\' ','n','s',\n",
    "           '’','t','v','big','mr','mrs','trump','r','p','new','year','day','month',\n",
    "           'g','week','state','b','j','u','donald','o','k','le','i','hi','other','e','da',\n",
    "           'st','e','m','full','f','g','h','i','l','q','r','t','x','z','last','order','rt',\n",
    "           'w','ka','wan','gen','way','y','d','rig','c','ra','ad','much','va','oh',\n",
    "           'one','level','leader','term','open','ax','eve','d','all','first','second',\n",
    "           'time','president','part','move','much','mark','act','el','don','side','reach','happen',\n",
    "           'thought','felt','would','easili','fabl','various','knew','say','came','togeth','explor',\n",
    "           'make','begin','record','must','set','ask','pundit','general','januari','alreadi','promis',\n",
    "           'onto','instead','away','becom','com','xbxd','number','mani','far','realli','longer','peopl',\n",
    "           'rememb','call','across','said','massiv','increas','dem','like','also','let','somebodi','respond',\n",
    "           'could','perhap','doesn','night','quick','ly','bit','http','busi','optim','surg','soar','gad','com','donaldtrump',\n",
    "           'lawenforcementappreciationday','facebook','intellig','agenc','tri','use','paid','total','witch','oppon',\n",
    "           'congrat','step','gone','get','even','made','rate','onto','morn','coupl','pres','wp','believ','twitter',\n",
    "           'realdonaldtrump','us','consid','lago','mar','three','ago','ag','bayer','pm','wasn','xl','oval','maga','deliv',\n",
    "           'facebook','html','long','short','two','everyon','like','wow','total','gotten','amaz','secur','assoc','seen','decis',\n",
    "           'talk','told','oval','fought','fa','today','hj','etc','wwboo','opufn','dr','met','happi','tri','presid','actual',\n",
    "           'done','dnc','th','got','anoth','met','org','vp','wh','co','mtgs','also','let','br','general','op','ed','te',\n",
    "           'great','vp','ron','keep','countri','look','news','help','thank','meet','leav','warm','welcom','man','asap','final',\n",
    "           'enjoy','fl','friday','visit','live','took','come','back','prior','brief','depart','spoke','zone','minut','unit',\n",
    "           'go','often','always','better','potenti','wait','mile','certain','nan','next','recent','larg','nc','sure']\n",
    "\n",
    "#stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "for tt in range(len(times_2days['text'])):\n",
    "    times_2days['text'][tt] = tknzr.tokenize(str(times_2days['text'][tt]).replace('u.s.','usa'))\n",
    "    times_2days['text'][tt] = [o for o in times_2days['text'][tt] if o not in stop_words]\n",
    "\n",
    "#명사만 추출\n",
    "for pos in range(len(times_2days['text'])):\n",
    "    pos_text=pos_tag(times_2days['text'][pos])\n",
    "    times_2days['text'][pos] = [q[0] for q in pos_text if q[1]=='NN'or q[1]=='NNS' or q[1]=='JJ']\n",
    "\n",
    "#트럼프 트윗과 겹치는 Times 데이터를 트윗 text에 추가하여 가중치 부여\n",
    "for trum in range(len(trump['text'])):\n",
    "    trump['text'][trum] = tknzr.tokenize(str(trump['text'][trum]).replace('u.s.','usa'))\n",
    "    trump['text'][trum] = [oo for oo in trump['text'][trum] if oo not in stop_words]\n",
    "    for trum2 in range(len(times_2days['text'][trum])):\n",
    "        if(times_2days['text'][trum][trum2] in trump['text'][trum]):\n",
    "            trump['text'][trum].append(times_2days['text'][trum][trum2])\n",
    "\n",
    "for plus in range(len(trump['text'])):\n",
    "    trump['text'][plus] = ' '.join(trump['text'][plus])\n",
    "\n",
    "trump.to_csv('weighted_Trump_Kospi_ver2.csv',mode='w')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  필터링 단어장 생성\n",
    "- times + forbes2000 + NASDAQ + DJIA + S&P500 + NYSE + Rusell2000 + 국가명 + 주요 도시 = deep러닝용 단어장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer ; tknzr = TweetTokenizer()\n",
    "from nltk.tag import pos_tag\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def lower_change(text):\n",
    "    return str(text).lower()\n",
    "\n",
    "#나스닥 상장회사 + 포브스 선정 2000대기업 + 산업군명\n",
    "nasdaq=pd.read_csv('NASDAQ.txt',sep='\\t') #출처 datahub\n",
    "forbes=pd.read_csv('Forbes.csv') #출처 Kaggle Forbes Top 2000 Companies\n",
    "\n",
    "nasdaq=nasdaq['Description']\n",
    "nas=[i.split() for i in nasdaq]\n",
    "\n",
    "stop = ['Inc','Gp','Cp','Com','CS','Grp','Ltd','Corp','I','Co','Corp.','Right','Unit','Hold','A','Inc.','Wt']\n",
    "\n",
    "company=[]\n",
    "for i in range(len(nas)):\n",
    "    box=[]\n",
    "    for j in nas[i]:\n",
    "        if(j not in stop):\n",
    "            box.append(j)\n",
    "        company.append(' '.join(box).lower())\n",
    "\n",
    "forbes=forbes[['Company','Sector','Industry']]\n",
    "forbes_sector=list(forbes['Sector'].apply(lower_change))\n",
    "forbes_industry=list(forbes['Industry'].apply(lower_change))\n",
    "forbes_company=[]\n",
    "fbs=[f.split() for f in forbes['Company']]\n",
    "for ii in range(len(fbs)):\n",
    "    box=[]\n",
    "    for jj in fbs[ii]:\n",
    "        if(jj not in stop):\n",
    "            box.append(jj)\n",
    "        forbes_company.append(' '.join(box).lower())\n",
    "\n",
    "forbes=forbes_company+forbes_sector+forbes_industry\n",
    "\n",
    "#미국 주, 도시 \n",
    "uscities = pd.read_csv('uscities.csv') #출처: simplemaps interactive data\n",
    "usa=uscities[['city','state_name','county_name']]\n",
    "city=list(set(usa['city']))\n",
    "state=list(set(usa['state_name']))\n",
    "county=list(set(usa['county_name']))\n",
    "america = city+state+county\n",
    "\n",
    "for x in range(len(america)):\n",
    "    america[x]= america[x].lower()\n",
    "\n",
    "#국가명, 도시명\n",
    "world = pd.read_csv('worldcities.csv')  #출처: simplemaps interactive data\n",
    "country=list(set(world['country']))\n",
    "country[20]='south korea'\n",
    "country[161]='north korea'\n",
    "\n",
    "for y in range(len(country)):\n",
    "    country[y]= country[y].lower()\n",
    "    \n",
    "city=list(set(world['city_ascii']))\n",
    "\n",
    "for z in range(len(city)):\n",
    "    city[z]= city[z].lower()\n",
    "\n",
    "#타임즈 키워드\n",
    "times=pd.read_csv('Times_data.csv')\n",
    "times['text']=times['text'].apply(lower_change)\n",
    "\n",
    "stop_words=['[',']','\\'','\\\"','\"\"','\\' ',',','.','\\'''','\\\\',':','-','\\' ','n','s',\n",
    "           '’','t','v','big','mr','mrs','trump','r','p','new','year','day','month',\n",
    "           'g','week','state','b','j','u','donald','o','k','le','i','hi','other','e','da',\n",
    "           'st','e','m','full','f','g','h','i','l','q','r','t','x','z','last','order','rt',\n",
    "           'w','ka','wan','gen','way','y','d','rig','c','ra','ad','much','va','oh','”','“','‘','“','put in'\n",
    "           'one','level','leader','term','open','ax','eve','d','all','first','second',\n",
    "           'time','president','part','move','much','mark','act','el','don','side','reach','happen','need','ms','mr','dr','years','moves'\n",
    "           'thought','felt','would','easili','fabl','various','knew','say','came','togeth','explor',\n",
    "           'make','begin','record','must','set','ask','pundit','general','januari','alreadi','promis',\n",
    "           'onto','instead','away','becom','com','xbxd','number','mani','far','realli','longer','peopl',\n",
    "           'rememb','call','across','said','massiv','increas','dem','like','also','let','somebodi','respond',\n",
    "           'could','perhap','doesn','night','quick','ly','bit','http','busi','optim','surg','soar','gad','com','donaldtrump',\n",
    "           'lawenforcementappreciationday','facebook','intellig','agenc','tri','use','paid','total','witch','oppon',\n",
    "           'congrat','step','gone','get','even','made','rate','onto','morn','coupl','pres','wp','believ','twitter',\n",
    "           'realdonaldtrump','us','consid','lago','mar','three','ago','ag','bayer','pm','wasn','xl','oval','maga','deliv',\n",
    "           'facebook','html','long','short','two','everyon','like','wow','total','gotten','amaz','secur','assoc','seen','decis',\n",
    "           'talk','told','oval','fought','fa','today','hj','etc','wwboo','opufn','dr','met','happi','tri','presid','actual',\n",
    "           'done','dnc','th','got','anoth','met','org','vp','wh','co','mtgs','also','let','br','general','op','ed','te',\n",
    "           'great','vp','ron','keep','countri','look','news','help','thank','meet','leav','warm','welcom','man','asap','final',\n",
    "           'enjoy','fl','friday','visit','live','took','come','back','prior','brief','depart','spoke','zone','minut','unit','answer',\n",
    "           'go','often','always','better','potenti','wait','mile','certain','nan','next','recent','larg','nc','sure','spirit','answers',\n",
    "           'animal','lists','start','men','days','sends','waters','tells','acting','kind','thanks','great','a','b','c','weeks','tweets',\n",
    "           'likes','view','didn','hopes','words','others','groups','seeks','feels','march','considers','brings','months','hundreds',\n",
    "           'thousands','caught','puts','names','group','friends','friend','others','moves','the','status','steve','book','see','agree',\n",
    "           'story','worker','workers','jobs','spirit','will','christmas','happy','great','people','place','good','show','nancy','new','nan']\n",
    "\n",
    "\n",
    "#stop_words에 해당하지 않는 데이터, 명사만 추출\n",
    "for tt in range(len(times['text'])):\n",
    "    times['text'][tt] = tknzr.tokenize(str(times['text'][tt]).replace('u.s.','usa'))\n",
    "    times['text'][tt] = [o for o in times['text'][tt] if o not in stop_words]\n",
    "    pos_text=pos_tag(times['text'][tt])\n",
    "    times['text'][tt] = [q[0] for q in pos_text if q[1]=='NN'or q[1]=='NNS']\n",
    "\n",
    "company=[com for com in company if com not in stop_words]\n",
    "america=[ame for ame in america if ame not in stop_words]\n",
    "country=[cou for cou in country if cou not in stop_words]\n",
    "city=[cit for cit in city if cit not in stop_words]\n",
    "\n",
    "times_text=[]\n",
    "for b in range(len(times['text'])):\n",
    "    times_text.extend(times['text'][b])\n",
    "times_text.append('dow')\n",
    "\n",
    "#중요도 필터링 단어장 생성\n",
    "twit_check_list= country+forbes+country+city\n",
    "twit_check_list=list(set(twit_check_list))\n",
    "\n",
    "twit_check_list.remove('new')\n",
    "\n",
    "'new' in twit_check_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시각화 코드 - EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "df = pd.read_csv('./Trumph_17_19.csv')\n",
    "\n",
    "df=df.iloc[:,]\n",
    "X_list=df['text'].tolist()\n",
    "\n",
    "content_text=''\n",
    "for each_line in X_list:\n",
    "    content_text = content_text + str(each_line) + '\\n'\n",
    "    \n",
    "content_text = content_text.replace('\\n','')\n",
    "content_text = content_text.replace(\"\\\\\",'') \n",
    "content_text = content_text.replace(\"\\'\",'\"')\n",
    "\n",
    "only_english = re.sub('[^a-zA-Z]', ' ', content_text)\n",
    "while '  ' in only_english:\n",
    "    only_english = only_english.replace('  ', ' ')\n",
    "\n",
    "no_capitals = only_english.lower().split()      \n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "stops.add('www')\n",
    "stops.add('https')\n",
    "\n",
    "no_stops = [word for word in no_capitals if not word in stops]\n",
    "\n",
    "no_stops_txt = nltk.Text(no_stops) \n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "no_stops_txt.plot(10)\n",
    "plt.show()\n",
    "\n",
    "data = no_stops_txt.vocab().most_common(300)\n",
    "wordcloud = WordCloud(font_path='c:/Windows/Fonts/malgun.ttf',\n",
    "                     relative_scaling=0.2, background_color='white'\n",
    "                     ).generate_from_frequencies(dict(data))\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델링 ( 순환 컨볼루션 레이어)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kosmo-20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kosmo-20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kosmo-20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kosmo-20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kosmo-20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Version :  2.2.4\n",
      "Tensorflow Version :  1.15.0\n",
      "Numpy Version :  1.17.4\n",
      "Matplotlib Version :  3.1.1\n",
      "Pandas Version :  0.25.3\n",
      "sklearn Version :  0.22.1\n",
      "Nltk Version :  3.4.5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer bidirectional_2 was called with an input that isn't a symbolic tensor. Received type: <class 'keras.engine.sequential.Sequential'>. Full input: [<keras.engine.sequential.Sequential object at 0x0000019EA991E888>]. All inputs to the layer should be tensors.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    278\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m                 \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mis_keras_tensor\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    473\u001b[0m         raise ValueError('Unexpectedly found an instance of type `' +\n\u001b[1;32m--> 474\u001b[1;33m                          \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m                          'Expected a symbolic tensor instance.')\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpectedly found an instance of type `<class 'keras.engine.sequential.Sequential'>`. Expected a symbolic tensor instance.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f53903b6849f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m \u001b[0mModel_deep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-f53903b6849f>\u001b[0m in \u001b[0;36mModel_deep\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;31m# 컨볼루션 레이어의 출력 이미지에서 주요 값만 뽑아 크기가 작은 출력 영상을 만듦. 지역적인 사소한 변화의 영향 방지\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(128,return_sequences=True,return_state=True,\n\u001b[1;32m--> 169\u001b[1;33m         recurrent_activation='relu',recurrent_initializer='glorot_uniform'))(model)\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;31m# ex) LSTM(3, input_dim=1, input_length=4)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;31m# 1번 인자 : 메모리 셀 갯수, 기억 용량의 정도와 출력 형태 결정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[1;31m# Applies the same workaround as in `RNN.__call__`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    412\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\MachineLearning\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    283\u001b[0m                                  \u001b[1;34m'Received type: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. Full input: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m                                  \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. All inputs to the layer '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m                                  'should be tensors.')\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Layer bidirectional_2 was called with an input that isn't a symbolic tensor. Received type: <class 'keras.engine.sequential.Sequential'>. Full input: [<keras.engine.sequential.Sequential object at 0x0000019EA991E888>]. All inputs to the layer should be tensors."
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Conv1D,MaxPooling1D\n",
    "from keras.preprocessing import sequence  # 기사별 단어 숫자 통일\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "# 기타 라이브러리\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# 토크나이저\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#모델 저장 라이브러리\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "#언어 분석 라이브러리\n",
    "from sklearn.pipeline import Pipeline #모델을 한꺼번에 저장해두고 사용\n",
    "from sklearn.metrics import accuracy_score # accuracy_score(y_test,y_pred) : y_test와 y_pred를 비교하고 값이 같은 비율, 즉 정확도를 계산\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "\n",
    "#워닝 없애기\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Keras Version : ', keras.__version__)\n",
    "print('Tensorflow Version : ', tf.__version__)\n",
    "print('Numpy Version : ', np.__version__)\n",
    "print('Matplotlib Version : ', matplotlib.__version__)\n",
    "print('Pandas Version : ', pd.__version__)\n",
    "print('sklearn Version : ', sklearn.__version__)\n",
    "print('Nltk Version : ', nltk.__version__)\n",
    "\n",
    "df = pd.read_csv('C:/Users/kosmo-20/Desktop/Project/code/data_final_1.csv')\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords') # stopwords 클래스를 사용 하기 위해 다운로드\n",
    "\n",
    "def text_transform(df): \n",
    "    text_new = [] # text_new 리스트 생성\n",
    "    for i in range(len(df)): # 데이터의 길이만큼 for문 돌림\n",
    "        df1 = df.iloc[i:i+1,] # df1 이라는 변수에 df의 각 행을 저장\n",
    "        text_list = df1['text'].tolist() # text_list 변수에 df1의 ''text칼럼'값 리스트 저장\n",
    "\n",
    "        content_text='' # content_text 라는 str타입 변수 생성\n",
    "        for each_line in text_list: # text_list 에서 for문\n",
    "            content_text = content_text + str(each_line) + '\\n' # content_text에 str타입으로 한 줄 씩 구분하여 추가\n",
    "        content_text = content_text.replace('\\n','')  # \\n제거\n",
    "        content_text = content_text.replace(\"\\\\\",'')  # \\\\ 제거\n",
    "        content_text = content_text.replace(\"\\'\",'\"')  # \\' 제거\n",
    "    \n",
    "        remove_list=[\"haven't\", 'haven\"t', \"hadn't\", 'hadn\"t', \"hasn't\", 'hasn\"t',\n",
    "                     \"didn't\", 'didn\"t', \"doesn't\", 'doesn\"t', \"don't\", 'don\"t',\n",
    "                     \"wasn't\", 'wasn\"t', \"weren't\", 'weren\"t', \"isn't\", 'isn\"t',\n",
    "                     \"aren't\", 'aren\"t', \"wouldn't\", 'wouldn\"t', \"won't\", 'won\"t',\n",
    "                     \"couldn't\", 'couldn\"t', \"shouldn't\", 'shouldn\"t', \"needn't\",\n",
    "                     'needn\"t', \"mightn't\", 'mightn\"t', \"mustn't\", 'mustn\"t', \n",
    "                     \"ain't\", 'ain\"t', \"shan't\", 'shan\"t', \"can't\", 'can\"t']\n",
    "                     # remove_list라는 변수에 부정어 저장\n",
    "    \n",
    "        for i in remove_list: # 부정어 list에서 for문\n",
    "            if i in content_text: #  부정어 list 안에 있는 value가 content_text에 있으면\n",
    "                content_text = content_text.replace(i , 'not') # content_text에 있는 부정어를 not으로 통일\n",
    "    \n",
    "        only_english = re.sub('[^a-zA-Z]', ' ', content_text) # only_english라는 변수에 content_text의 영어 알파벳 형식만 저장\n",
    "    \n",
    "        while '  ' in only_english: # only_english에 \\t 가 있으면\n",
    "            only_english = only_english.replace('  ', ' ') # 화이트 스페이스로 계속 교체함\n",
    "        \n",
    "        no_capitals = only_english.lower().split() # only_english를 소문자화해서 공백으로 분할한 값을 no_caplitals에 저장\n",
    "    \n",
    "        stops = set(stopwords.words('english')) # stopwords 클래스의 객체 stops 생성\n",
    "        stops.add('www') # stops에 'www' 추가\n",
    "        stops.add('https') # stops에 'https' 추가\n",
    "        stops.remove('not') # stops에 not 제거 -> not은 제거하지 말고 살림\n",
    "    \n",
    "        no_stops = [word for word in no_capitals if not word in stops]\n",
    "        # no_capitals의 value들 중 stops의 value와 겹치지 않는 values를 no_stops에 저장함\n",
    "    \n",
    "        stemmer = nltk.stem.SnowballStemmer('english') # stem 어간 추출\n",
    "        stemmer_words = [stemmer.stem(word) for word in no_stops] # 불용어 처리가 끝난 text를 저장한 no_stops에 있는 values의 어간을 추출하여 list에 넣어줌\n",
    "    \n",
    "        text_join = ' '.join(stemmer_words) # stemmer_words 를 합쳐줌 \n",
    "        text_new.append(text_join) # text_new에 text_join을 추가함\n",
    "\n",
    "    df['text'] = text_new # 불용어 처리가 끝난 text_new를 기존 df의 text 칼럼에 저장\n",
    "    \n",
    "    \n",
    "text_transform(df)\n",
    "\n",
    "# 모델 구축\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, Conv1D, MaxPooling1D, LSTM, Dense, Flatten,Bidirectional,BatchNormalization,Concatenate\n",
    "\n",
    "def Model_deep(data):\n",
    "    index_fund = ['KRX_건설','KRX_경기소비재','KRX_기계장비','KRX_미디어엔터',\n",
    "                  'KRX_반도체','KRX_방송통신','KRX_보험','KRX_에너지','KRX_운송',\n",
    "                  'KRX_유틸리티','KRX_은행','KRX_자동차','KRX_정보기술','KRX_증권',\n",
    "                  'KRX_철강','KRX_필수소비재','KRX_헬스','KRX300_금융',\n",
    "                  'KRX300_산업재','KRX300_소재','KRX300_자유소비재',\n",
    "                  'KRX300_정보기술','KRX300_커뮤','KRX300_필수소비재','KRX300_헬스']\n",
    "    # index_fund에 krx섹터지수 리스트 저장\n",
    "    Model_Acc = [] # Model_Acc라는 리스트 생성\n",
    "    tok = Tokenizer(num_words=1000)  # 빈도 수 상위 1000개 단어를 대상으로 하는 Tokenizer 클래스 객체 생성\n",
    "  \n",
    "    for A in index_fund:\n",
    "        df=data[['text',A]] # df에 text칼럼과 krx섹터지수 칼럼 저장\n",
    "        tok.fit_on_texts(df['text']) # text 칼럼 values들에 인덱스를 추가함\n",
    "        sequences = tok.texts_to_sequences(df['text']) # 위의 작업 결과를 토대로 벡터화하여 sequences라는 변수에 저장\n",
    "        \n",
    "        #train set, test set 구분\n",
    "        target_list = df[A].tolist() # df의 krx섹터지수 칼럼들의 values를 target_list라는 변수에 저장\n",
    "        text_train, text_test, target_train, target_test = train_test_split( # sequences를 text_train,test로, target_list를 target_train,test로 데이터 분리)\n",
    "            sequences,target_list, test_size=0.3,random_state=0) \n",
    "          \n",
    "        #데이터 개수 맞춰주기, 553개 행의 각 데이터 개수를 220개로 지정\n",
    "        text_train = sequence.pad_sequences(text_train, maxlen=220) # 데이터를 220개 단어로 잘라서 2차원 numpy 배열로 만들어줌\n",
    "        text_test = sequence.pad_sequences(text_test, maxlen=220)\n",
    "\n",
    "        # One_hot_encoding\n",
    "        # target_train = np_utils.to_categorical(target_train) # target_train 벡터(integer) 값을 2진 매트릭스로 변환 -> multi classfication -> softmax -> categorical_crossentropy\n",
    "        # target_test = np_utils.to_categorical(target_test) # target_test 벡터(integer) 값을 2진 매트릭스로 변환\n",
    "\n",
    "        #모델 생성\n",
    "        model = Sequential() # 층 쌓는 모델 생성\n",
    "        model.add(Embedding(1000,64,input_length=220)) # 벡터크기를 지정해주지 않아도 자동으로 적절한 크기를 찾아줌. Embedding(max_feature, 벡터크기, maxlen)\n",
    "        # 단어를 의미론적 기하공간에 매핑할 수 있도록 벡터화시킵니다.\n",
    "        # 첫번째 인자(input_dim) : 단어 사전의 크기를 말하며 총 20,000개의 단어 종류가 있다는 의미입니다. 이 값은 앞서 imdb.load_data() 함수의 num_words 인자값과 동일해야 합니다.\n",
    "        # 두번째 인자(output_dim) : 단어를 인코딩 한 후 나오는 벡터 크기 입니다.  이 값이 128이라면 단어를 128차원의 의미론적 기하공간에 나타낸다는 의미입니다. \n",
    "        # 단순하게 빈도수만으로 단어를 표시한다면, 10과 11은 빈도수는 비슷하지만 단어로 볼 때는 전혀 다른 의미를 가지고 있습니다. 하지만 의미론적 기하공간에서는 거리가 가까운 두 단어는 의미도 유사합니다. \n",
    "        # 즉 임베딩 레이어는 입력되는 단어를 의미론적으로 잘 설계된 공간에 위치시켜 벡터로 수치화 시킨다고 볼 수 있습니다.\n",
    "        # input_length : 단어의 수 즉 문장의 길이를 나타냅니다. 임베딩 레이어의 출력 크기는 샘플 수 * output_dim * input_lenth가 됩니다. \n",
    "        # 임베딩 레이어 다음에 Flatten 레이어가 온다면 반드시 input_lenth를 지정해야 합니다. 플래튼 레이어인 경우 입력 크기가 알아야 이를 1차원으로 만들어서 Dense 레이어에 전달할 수 있기 때문입니다.\n",
    "        # model.add(Dropout(0.5))\n",
    "        model.add(Conv1D(64,5, padding='same', activation='relu', strides=1))\n",
    "        # 필터로 특징을 추출하는 컨볼루션 레이어\n",
    "        # 1번 인자 : 컨볼루션 필터의 수\n",
    "        # 2번 인자 : 커널의 크기\n",
    "        # padding : 경계 처리 방법 (valid or same)\n",
    "        # input_shape : 샘플 수를 제외한 입력 형태, 첫 레이어일 때만 정의 (행,열,채널수) 흑백: 채널=1 컬러:채널=3\n",
    "        model.add(MaxPooling1D(pool_size=3))\n",
    "        # 컨볼루션 레이어의 출력 이미지에서 주요 값만 뽑아 크기가 작은 출력 영상을 만듦. 지역적인 사소한 변화의 영향 방지\n",
    "        model(add(LSTM(64)))\n",
    "        # ex) LSTM(3, input_dim=1, input_length=4)\n",
    "        # 1번 인자 : 메모리 셀 갯수, 기억 용량의 정도와 출력 형태 결정\n",
    "        # input_dim : 입력 속성 수, 일반적으로 속성의 갯수가 들어감\n",
    "        # input_length : 시퀀스 데이터의 입력 길이 (여기서는 220)\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        # output layer는 binary(0,1), sigmoid\n",
    "        #학습\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam',  # binary_crossentropy 대신 focal 혹은 다른 것들도 고려해볼 것\n",
    "                      metrics=['accuracy'])\n",
    "        model.fit(text_train, target_train, batch_size = 100, \n",
    "                  validation_split=(0.1), epochs = 15, verbose=0)\n",
    "        Acc = model.evaluate(text_test, target_test)[1] # 정확도 평가\n",
    "    \n",
    "        url = 'Sector_' + A + '.h5'\n",
    "    \n",
    "        print('Model : ', url)\n",
    "        print('Accuracy : %.4f' %Acc)\n",
    "       \n",
    "        model.save(url)\n",
    "        print('저장 완료')\n",
    "    \n",
    "    \n",
    "Model_deep(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 오늘의 twit 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def text_transform_today(tweet_today):\n",
    "    remove_list=[\"haven't\", 'haven\"t', \"hadn't\", 'hadn\"t', \"hasn't\", 'hasn\"t',\n",
    "                 \"didn't\", 'didn\"t', \"doesn't\", 'doesn\"t', \"don't\", 'don\"t',\n",
    "                 \"wasn't\", 'wasn\"t', \"weren't\", 'weren\"t', \"isn't\", 'isn\"t',\n",
    "                 \"aren't\", 'aren\"t', \"wouldn't\", 'wouldn\"t', \"won't\", 'won\"t',\n",
    "                 \"couldn't\", 'couldn\"t', \"shouldn't\", 'shouldn\"t', \"needn't\",\n",
    "                 'needn\"t', \"mightn't\", 'mightn\"t', \"mustn't\", 'mustn\"t', \"ain't\",\n",
    "                 'ain\"t', \"shan't\", 'shan\"t', \"can't\", 'can\"t']\n",
    "    for i in remove_list:\n",
    "        if i in tweet_today:\n",
    "            content_text = tweet_today.replace(i, 'not')\n",
    "        else:\n",
    "            content_text = tweet_today\n",
    "    \n",
    "    only_english = re.sub('[^a-zA-Z]', ' ', content_text)\n",
    "    \n",
    "    while '  ' in only_english:\n",
    "        only_english = only_english.replace('  ', ' ')\n",
    "        \n",
    "    no_capitals = only_english.lower().split()\n",
    "    \n",
    "    stops = set(stopwords.words('english'))\n",
    "    stops.add('www')\n",
    "    stops.add('https')\n",
    "    stops.add('nan')\n",
    "    stops.remove('not')\n",
    "    \n",
    "    no_stops = [word for word in no_capitals if not word in stops]\n",
    "    \n",
    "    stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    stemmer_words = [stemmer.stem(word) for word in no_stops]\n",
    "    \n",
    "    text_join = ' '.join(stemmer_words)\n",
    "    return text_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df2020 = pd.read_csv('./data_final_2020 (1).csv')\n",
    "tweet_today = df2020['text']\n",
    "\n",
    "transform_tweet = text_transform_today(tweet_today)\n",
    "transform_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from wordcloud import STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def transform():\n",
    "    only_english = re.sub('[^a-zA-Z]', ' ', tweet_today)\n",
    "    no_capitals = only_english.lower().split()\n",
    "    stops = set(stopwords.words('english'))\n",
    "    remove_list=[\"haven't\", \"wouldn't\", \"couldn't\", 'ain', 'couldn', 'shouldn',\n",
    "                 \"hadn't\", 'doesn', 'mightn', \"doesn't\", 'don', 'mustn', 'haven',\n",
    "                 'isn', 'weren', \"mightn't\", 'didn', \"isn't\", 'not', \"shouldn't\", \n",
    "                 \"hasn't\", 'aren', \"wasn't\", 'wasn', \"shan't\", 'hasn', \"don't\", \n",
    "                 \"didn't\", \"mustn't\", 'hadn', 'wouldn', 'needn', \"weren't\", \n",
    "                 \"needn't\", \"won't\", \"aren't\"]\n",
    "    for i in remove_list:\n",
    "        stops.remove(i)\n",
    "    stops.add('www')\n",
    "    stops.add('https')\n",
    "    no_stops = [word for word in no_capitals if not word in stops]\n",
    "    stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    stemmer_words = [stemmer.stem(word) for word in no_stops]\n",
    "    text_join = ' '.join(stemmer_words)\n",
    "    return text_join\n",
    "\n",
    "\n",
    "\n",
    "def Using_model():\n",
    "  Sector = pd.DataFrame(columns= ['Sector','Accuracy','result'])\n",
    "  \n",
    "  index_fund = ['KRX_건설','KRX_경기소비재','KRX_기계장비','KRX_미디어엔터',\n",
    "                'KRX_반도체','KRX_방송통신','KRX_보험','KRX_에너지','KRX_운송',\n",
    "                'KRX_유틸리티','KRX_은행','KRX_자동차','KRX_정보기술','KRX_증권',\n",
    "                'KRX_철강','KRX_필수소비재','KRX_헬스','KRX300_금융','KRX300_산업재',\n",
    "                'KRX300_소재','KRX300_자유소비재','KRX300_정보기술','KRX300_커뮤',\n",
    "                'KRX300_필수소비재','KRX300_헬스']\n",
    "\n",
    "  text = input('또람프 트위터 메세지 : ')  # 인풋 메세지\n",
    "  string = list(transform(text))\n",
    "\n",
    "  for i in index_fund:\n",
    "    url = 'Sector_' + i + '.h5'\n",
    "    \n",
    "    model = load_model(url)\n",
    "\n",
    "    # 예측 정확도(r1)\n",
    "    r1 = np.max(model.predict_proba(string)*100) # 확률 값에 *100 해줌\n",
    "        \n",
    "    # 예측 결과(r2)\n",
    "    r2 = model.predict(string)[0]  # 긍정 --> 1, 부정 --> 0이 출력\n",
    "        \n",
    "    if r2 == 1: #긍정이냐?(4~5점)\n",
    "      df_1= pd.DataFrame({'Sector': [i] ,'Accuracy':[r1],'result' : ['1']})\n",
    "      Sector = Sector.append(df_1)\n",
    "    else :        #부정이냐?(1~3점)\n",
    "      df_2= pd.DataFrame({'Sector': [i] ,'Accuracy': [r1],'result' : ['0']})\n",
    "      Sector = Sector.append(df_2)\n",
    "  return Sector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텔레그램 전송"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sending_text():\n",
    "    data=Using_model()\n",
    "    up = data.loc[data['result']=='1',['Sector','Accuracy']]\n",
    "    up = up.sort_values(by=['Accuracy'], axis=0, ascending=False)\n",
    "    up = up[:3]\n",
    "    down = data.loc[data['result']=='0', ['Sector','Accuracy']]\n",
    "    down = down.sort_values(by=['Accuracy'], axis=0, ascending=True)\n",
    "    down = down[:3]\n",
    "    up_list = []\n",
    "    down_list = []\n",
    "    for i in range(3):\n",
    "        up_list.append(\"{0}\".format(up.iloc[i,0]))\n",
    "    for l in range(3):\n",
    "        down_list.append(\"{0}\".format(down.iloc[l,0]))\n",
    "    down_str = '\\n'.join(down_list)\n",
    "    up_str = '\\n'.join(up_list)\n",
    "\n",
    "    import datetime\n",
    "    aaa=str(datetime.datetime.now().date().strftime('%Y-%m-%d')) # 날짜를 str타입 출력\n",
    "    text_message = aaa + \"\\n 새로운 트윗입니다\\n\"+\"트윗내용 : \\n\" + tweet_today+'\\n **상승 종목**\\n'+up_str+\"\\n **하강 종목**\\n\"+down_str            \n",
    "    return text_message\n",
    "\n",
    "import telegram\n",
    "\n",
    "def Telegram():\n",
    "    text_message = sending_text()\n",
    "    telgm_token = ['939273649:AAF0ZaO_itBE1zUlgxAiUebOZReb-J_Dthk']\n",
    "    # 각자 Bot의 API KEY\n",
    "    # 각자 Bot을 트럼프카드 채널에 입장시킨 후 관리자 등록 후 동작\n",
    "    bot = telegram.Bot(token = telgm_token) #  봇key를 변수 bot에 저장\n",
    "    updates = bot.getUpdates() # 롱-폴링 기법을 사용하여 업데이트를 받음\n",
    "    bot.sendMessage(chat_id='@trumph_card', text=text_message)\n",
    "    #chat_id = 채팅 룸의 chat_id, text=\"보낼 메세지 입력\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 번외) TFIDF 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(data):\n",
    "    index_fund = ['KRX_건설','KRX_경기소비재','KRX_기계장비','KRX_미디어엔터',\n",
    "                'KRX_반도체','KRX_방송통신','KRX_보험','KRX_에너지','KRX_운송',\n",
    "                'KRX_유틸리티','KRX_은행','KRX_자동차','KRX_정보기술','KRX_증권',\n",
    "                'KRX_철강','KRX_필수소비재','KRX_헬스','KRX300_금융','KRX300_산업재',\n",
    "                'KRX300_소재','KRX300_자유소비재','KRX300_정보기술','KRX300_커뮤',\n",
    "                'KRX300_필수소비재','KRX300_헬스']\n",
    "    \n",
    "    for A in index_fund:\n",
    "        df=data[['text',A]]\n",
    "        text_list = df['text'].tolist()\n",
    "        target_list = df[A].tolist()\n",
    "\n",
    "        text_train, text_test, target_train, target_test = train_test_split(text_list,target_list, test_size=0.2,random_state=0) \n",
    "        \n",
    "        tfidf = TfidfVectorizer(lowercase = False, tokenizer = tok)\n",
    "    \n",
    "        logistic = LogisticRegression(C= 10,penalty= 'l2', random_state= 0 )\n",
    "\n",
    "        pipe = Pipeline([('vect',tfidf),('clf',logistic)])\n",
    "    \n",
    "        pipe.fit(text_train,target_train)\n",
    "\n",
    "        y_pred = pipe.predict(text_test)\n",
    "        \n",
    "        url = 'Sector_' + A + '.h5'\n",
    "        \n",
    "        print('Model : ', url)\n",
    "        print(accuracy_score(target_test, y_pred))\n",
    "        \n",
    "        with open(url,'wb') as fp:\n",
    "            pickle.dump(pipe,fp)\n",
    "            \n",
    "            \n",
    "\n",
    "def Using_model():\n",
    "    Sector = pd.DataFrame(columns= ['Sector','Accuracy','result'])\n",
    "  \n",
    "    index_fund = ['KRX_건설','KRX_경기소비재','KRX_기계장비','KRX_미디어엔터',\n",
    "                'KRX_반도체','KRX_방송통신','KRX_보험','KRX_에너지','KRX_운송',\n",
    "                'KRX_유틸리티','KRX_은행','KRX_자동차','KRX_정보기술','KRX_증권',\n",
    "                'KRX_철강','KRX_필수소비재','KRX_헬스','KRX300_금융','KRX300_산업재',\n",
    "                'KRX300_소재','KRX300_자유소비재','KRX300_정보기술','KRX300_커뮤',\n",
    "                'KRX300_필수소비재','KRX300_헬스']\n",
    "\n",
    "    text = input('또람프 트위터 메세지 : ')  # 인풋 메세지\n",
    "    string = [text]\n",
    "\n",
    "    for i in index_fund:\n",
    "        url = 'Sector_' + i + '.h5'\n",
    "        with open(url,'rb') as fp:\n",
    "              pipe = pickle.load(fp)\n",
    "\n",
    "        # 예측 정확도(r1)\n",
    "        r1 = np.max(pipe.predict_proba(string)*100) # 확률 값에 *100 해줌\n",
    "        \n",
    "        # 예측 결과(r2)\n",
    "        r2 = pipe.predict(string)[0]  # 긍정 --> 1, 부정 --> 0이 출력\n",
    "        \n",
    "        if r2 == 1: #긍정이냐?(4~5점)\n",
    "            df_1= pd.DataFrame({'Sector': [i] ,'Accuracy':[r1],'result' : ['1']})\n",
    "            Sector = Sector.append(df_1)\n",
    "        else :        #부정이냐?(1~3점)\n",
    "            df_2= pd.DataFrame({'Sector': [i] ,'Accuracy': [r1],'result' : ['0']})\n",
    "            Sector = Sector.append(df_2)\n",
    "    return Sector.to_csv('Sector_predict.csv', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 번외) 컨볼루션  모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words=1000\n",
    "tk = data[['text','KRX_건설']]\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(tk['text'])\n",
    "sequences = tokenizer.texts_to_sequences(tk['text'])\n",
    "target_list = data['KRX_건설'].tolist()\n",
    "\n",
    "# 원-핫-=해싱 기법\n",
    "#ont_hot_results = tokenizer.texts_to_matrix(tk['text'],mode='binary')\n",
    "# 고유 토큰 찾기\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "# 가능한 토큰의 갯수 ( 단어 인덱스 최댓값 +1), 입베딩차원(여기서는 64)\n",
    "# embedding_layer = Embedding(1000,64) \n",
    "\n",
    "max_features = 1000 # 특성으로 사용할 단어의 수\n",
    "maxlen=220 # 사용할 텍스트의 길이(가장 빈번한 max_features 개의 단어만 사용)\n",
    "\n",
    "# 데이터 스플릿\n",
    "x_train, x_test, y_train, y_test = train_test_split(sequences, target_list, test_size=0.3, random_state=0)\n",
    "\n",
    "# 리스트를 (sample, maxlen) 크기의 2D 정수 텐서로 변환\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten,Dense,Embedding\n",
    "\n",
    "model = Sequential()\n",
    "# 나중에 임베딩된 입력을 Flatten에서 펼치기 위해 Embedding 층에 input_length 지정\n",
    "model.add(Embedding(max_features,16,input_length=maxlen))\n",
    "model.add(Conv1D(32,5, activiation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=10, validation_data=(x_test,y_test))\n",
    "# 모델 평가\n",
    "print(\"Accuracy :\",model.evaluate(x_test,y_test)[1])\n",
    "# 모델 저장\n",
    "model.save('Embedding1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 번외) 순환 신경망 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구축\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, Conv1D, MaxPooling1D, LSTM, Dense, Flatten,Bidirectional,BatchNormalization,Concatenate\n",
    "\n",
    "def Model_deep(data):\n",
    "    index_fund = ['KRX_건설','KRX_경기소비재','KRX_기계장비','KRX_미디어엔터',\n",
    "                  'KRX_반도체','KRX_방송통신','KRX_보험','KRX_에너지','KRX_운송',\n",
    "                  'KRX_유틸리티','KRX_은행','KRX_자동차','KRX_정보기술','KRX_증권',\n",
    "                  'KRX_철강','KRX_필수소비재','KRX_헬스','KRX300_금융',\n",
    "                  'KRX300_산업재','KRX300_소재','KRX300_자유소비재',\n",
    "                  'KRX300_정보기술','KRX300_커뮤','KRX300_필수소비재','KRX300_헬스']\n",
    "    # index_fund에 krx섹터지수 리스트 저장\n",
    "    Model_Acc = [] # Model_Acc라는 리스트 생성\n",
    "    tok = Tokenizer(num_words=1000)  # 빈도 수 상위 1000개 단어를 대상으로 하는 Tokenizer 클래스 객체 생성\n",
    "  \n",
    "    for A in index_fund:\n",
    "        df=data[['text',A]] # df에 text칼럼과 krx섹터지수 칼럼 저장\n",
    "        tok.fit_on_texts(df['text']) # text 칼럼 values들에 인덱스를 추가함\n",
    "        sequences = tok.texts_to_sequences(df['text']) # 위의 작업 결과를 토대로 벡터화하여 sequences라는 변수에 저장\n",
    "        \n",
    "        #train set, test set 구분\n",
    "        target_list = df[A].tolist() # df의 krx섹터지수 칼럼들의 values를 target_list라는 변수에 저장\n",
    "        text_train, text_test, target_train, target_test = train_test_split( # sequences를 text_train,test로, target_list를 target_train,test로 데이터 분리)\n",
    "            sequences,target_list, test_size=0.3,random_state=0) \n",
    "          \n",
    "        #데이터 개수 맞춰주기, 553개 행의 각 데이터 개수를 220개로 지정\n",
    "        text_train = sequence.pad_sequences(text_train, maxlen=220) # 데이터를 220개 단어로 잘라서 2차원 numpy 배열로 만들어줌\n",
    "        text_test = sequence.pad_sequences(text_test, maxlen=220)\n",
    "\n",
    "        # One_hot_encoding\n",
    "        # target_train = np_utils.to_categorical(target_train) # target_train 벡터(integer) 값을 2진 매트릭스로 변환 -> multi classfication -> softmax -> categorical_crossentropy\n",
    "        # target_test = np_utils.to_categorical(target_test) # target_test 벡터(integer) 값을 2진 매트릭스로 변환\n",
    "\n",
    "        #모델 생성\n",
    "        model = Sequential() # 층 쌓는 모델 생성\n",
    "        model.add(Embedding(1000,64,input_length=220)) # 벡터크기를 지정해주지 않아도 자동으로 적절한 크기를 찾아줌. Embedding(max_feature, 벡터크기, maxlen)\n",
    "        # 단어를 의미론적 기하공간에 매핑할 수 있도록 벡터화시킵니다.\n",
    "        # 첫번째 인자(input_dim) : 단어 사전의 크기를 말하며 총 20,000개의 단어 종류가 있다는 의미입니다. 이 값은 앞서 imdb.load_data() 함수의 num_words 인자값과 동일해야 합니다.\n",
    "        # 두번째 인자(output_dim) : 단어를 인코딩 한 후 나오는 벡터 크기 입니다.  이 값이 128이라면 단어를 128차원의 의미론적 기하공간에 나타낸다는 의미입니다. \n",
    "        # 단순하게 빈도수만으로 단어를 표시한다면, 10과 11은 빈도수는 비슷하지만 단어로 볼 때는 전혀 다른 의미를 가지고 있습니다. 하지만 의미론적 기하공간에서는 거리가 가까운 두 단어는 의미도 유사합니다. \n",
    "        # 즉 임베딩 레이어는 입력되는 단어를 의미론적으로 잘 설계된 공간에 위치시켜 벡터로 수치화 시킨다고 볼 수 있습니다.\n",
    "        # input_length : 단어의 수 즉 문장의 길이를 나타냅니다. 임베딩 레이어의 출력 크기는 샘플 수 * output_dim * input_lenth가 됩니다. \n",
    "        # 임베딩 레이어 다음에 Flatten 레이어가 온다면 반드시 input_lenth를 지정해야 합니다. 플래튼 레이어인 경우 입력 크기가 알아야 이를 1차원으로 만들어서 Dense 레이어에 전달할 수 있기 때문입니다.\n",
    "        \n",
    "        model.add(LSTM(64))\n",
    "        # ex) LSTM(3, input_dim=1, input_length=4)\n",
    "        # 1번 인자 : 메모리 셀 갯수, 기억 용량의 정도와 출력 형태 결정\n",
    "        # input_dim : 입력 속성 수, 일반적으로 속성의 갯수가 들어감\n",
    "        # input_length : 시퀀스 데이터의 입력 길이 (여기서는 220)\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        # output layer는 binary, sigmoid\n",
    "        \n",
    "        #학습\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam',  # binary_crossentropy 대신 focal 혹은 다른 것들도 고려해볼 것\n",
    "                      metrics=['accuracy'])\n",
    "        model.fit(text_train, target_train, batch_size = 100, \n",
    "                  validation_split=(0.1), epochs = 15, verbose=0)\n",
    "        Acc = model.evaluate(text_test, target_test)[1] # 정확도 평가\n",
    "    \n",
    "        url = 'Sector_' + A + '.h5'\n",
    "    \n",
    "        print('Model : ', url)\n",
    "        print('Accuracy : %.4f' %Acc)\n",
    "       \n",
    "        model.save(url)\n",
    "        print('저장 완료')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 번외) 바나다우 어텐션 메커니즘을 적용한 양방향 순환 신경망 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = Dense(units)\n",
    "    self.W2 = Dense(units)\n",
    "    self.V = Dense(1)\n",
    "\n",
    "  def call(self, values, query): # 단, key와 value는 같음\n",
    "    # hidden shape == (batch_size, hidden size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # we are doing this to perform addition to calculate the score\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, BatchNormalization\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import optimizers\n",
    "import os\n",
    "\n",
    "sequence_input = Input(shape=(220,))\n",
    "embedded_sequences = Embedding(vocab_size, 128, input_length=max_len)(sequence_input)\n",
    "\n",
    "lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional \\\n",
    "    (LSTM\n",
    "     (128,\n",
    "      dropout=0.3,\n",
    "      return_sequences=True,\n",
    "      return_state=True,\n",
    "      recurrent_activation='relu',\n",
    "      recurrent_initializer='glorot_uniform'))(embedded_sequences)\n",
    "\n",
    "state_h = Concatenate()([forward_h, backward_h]) # 은닉 상태\n",
    "state_c = Concatenate()([forward_c, backward_c]) # 셀 상태\n",
    "\n",
    "attention = BahdanauAttention(128) # 가중치 크기 정의\n",
    "context_vector, attention_weights = attention(lstm, state_h)\n",
    "\n",
    "hidden = BatchNormalization()(context_vector)\n",
    "\n",
    "output = Dense(2, activation='softmax')(hidden)\n",
    "model = Model(inputs=sequence_input, outputs=output)\n",
    "Adam = optimizers.Adam(lr=0.0001, clipnorm=1.)\n",
    "model.compile(optimizer=Adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(text_train, target_train, epochs=10, batch_size=128, validation_data=(text_test, target_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kosmo-20\\\\Desktop\\\\Project\\\\code'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
