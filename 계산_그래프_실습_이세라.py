# -*- coding: utf-8 -*-
"""계산 그래프 실습_이세라.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11xV2Ba5vgVKyF5N2OSLKkqvI311mjFMI
"""

#곱셈 그래프 
class MulLayer:
  def _init_(self):
    self.x = None
    self.y = None

  def forward(self,x,y):
    self.x = x
    self.y = y
    out = x*y
    return out

  def backward(self,dout):
    dx = dout * self.y
    dy = dout * self.x
    return dx, dy

#덧셈 그래프
class AddLayer:
  def _init_(self):
    pass
  
  def forward(self,x,y):
    out = x+y
    return out

  def backward(self,dout):
    dx = dout*1
    dy = dout*1
    return dx,dy

apple = 100
apple_num = 2
orange = 150
orange_num = 3
tax = 1.1

mul_apple_graph = MulLayer()
mul_orange_graph = MulLayer()
add_apple_orange_graph = AddLayer()
mul_tax_graph = MulLayer()

apple_price = mul_apple_graph.forward(apple,apple_num)
orange_price = mul_orange_graph.forward(orange,orange_num)
all_price = add_apple_orange_graph.forward(apple_price,orange_price)
price=mul_tax_graph.forward(all_price,tax)

dprice = 1
dall_price,dtax = mul_tax_graph.backward(dprice)
dapple_price,dorange_price = add_apple_orange_graph.backward(dall_price)
dorange,dorange_num = mul_orange_graph.backward(dorange_price)
dapple,dapple_num = mul_apple_graph.backward(dapple_price)

print('price: ',int(price))
print('dApple:', dapple)
print('dApple_num:',int(dapple_num))
print('dOrange:',dorange)
print('dOrnage_num:',dorange_num)

print(price)
# 양자화애러 , 이진수를 사용해서 생김

#Sigmoid 그래프
class Sigmoid:
  def __init__(self):
    self.out = None
  
  def forward(self,x):
    out = 1/(1+np.exp(-x))
    self.out = out
    return out

  def backward(self,dout):
    dx = dout*(1.0-self.out)*self.out  # 미분값 x(1-x)
    return dx

# 행렬곱 연산
class Affine:
  def __init__(self,W,b):
    self.W = W
    self.b = b
    self.x = None
    self.dW = None
    self.db = None

  def forward(self,x):
    self.x = x
    out = np.dot(x,self.W) +self.b
    return out

  def backward(self,dout):
    dx = np.dot(dout,self.W.T)
    self.dW = np.dot(self.x.T,dout)
    self.db = np.sum(dout,axis = 0)

    return dx

class MSE:
  def __init__(self):
    self.loss = None
    self.y= None
    self.t = None
  def forward(self,x,t):
    self.t = t
    self.y = x
    self.loss = 1/2 *np.square(self.t - self.y).sum()
    return self.loss

  def backward(self,dout=1):
    batch_size = self.t.shape[0]
    dx = (self.y - self.t)/batch_size
    return dx

import numpy as np
from collections import OrderedDict
import matplotlib.pyplot as plt

class TwoLayerNet:
  def __init__(self, input_size, hidden_size, output_size):
    self.params = {}
    self.params['W1'] = np.random.uniform(size=(input_size, hidden_size))
    self.params['b1'] = np.zeros(hidden_size)
    self.params['W2'] = np.random.uniform(size=(hidden_size, output_size))
    self.params['b2'] = np.zeros(output_size)

    self.layers = OrderedDict()
    self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
    self.layers['Sigmoid'] = Sigmoid()
    self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
    self.layers['Sigmoid2'] = Sigmoid()
    self.lastLayer = MSE()

  def predict(self, x):
    for layer in self.layers.values():
      x = layer.forward(x)
    return x
  
  def loss(self, x, t):
    y = self.predict(x)
    return self.lastLayer.forward(y, t)

  def accuracy(self, x, t):
    y = self.predict(x)
    y = np.argmax(y, axis=1)
    if t.ndim != 1:
      t = np.argmax(t, axis=1)

    accuracy = np.sum(y==t)/float(x.shape[0])
    return accuracy

  def gradient(self, x, t):
    #forward
    self.loss(x, t)
    #backward
    dout = 1
    dout = self.lastLayer.backward(dout)
    layers = list(self.layers.values())
    layers.reverse()
    for layer in layers:
      dout = layer.backward(dout)
    
    grads = {}
    grads['W1'] = self.layers['Affine1'].dW
    grads['b1'] = self.layers['Affine1'].db
    grads['W2'] = self.layers['Affine2'].dW
    grads['b2'] = self.layers['Affine2'].db
    return grads

X = np.array([[0,0],[0,1],[1,0],[1,1]])
t = np.array([[0],[1],[1],[1]])

network = TwoLayerNet(input_size=2, hidden_size=3, output_size=1)

epoch = 20000

learning_rate = 0.1

train_loss_list = []
for i in range(epoch):
  grad = network.gradient(X, t)

  for key in ('W1', 'b1', 'W2', 'b2'):
    network.params[key] -= learning_rate * grad[key]
   
  loss = network.loss(X,t)
  train_loss_list.append(loss)

plt.plot(train_loss_list)
plt.show()
























